确定数据同步任务所有可能的 JOB 入口:
    1. aml_bdp/bin/utils/python_utils/etlTool.py | HAML_PYTHON_ETL_TOOL --type=spark
    2. aml_bdp/bin/utils/bash_utils/etl_submit.sh | ETL_SUBMIT --type=sqoop
    3. aml_bdp/bin/utils/bash_utils/etl_submit.sh | ETL_SUBMIT --type=spark
    4. aml_bdp/bin/utils/bash_utils/etl_submit.sh | ETL_SUBMIT --type=blanca
        1. Sink-SparkJob
        2. Source-Jdbc-dbInstance, Sink-Jdbc-dbInstance
    5. aml_bdp/bin/utils/python_utils/Sqoop.py | HAML_PYTHON_SQOOP_TOOL
    6. 其他脚本文件作为入口
        如: sqoop/import/blacklist/operator/import_mlist_type.py

确定数据同步任务包含的 JOB 关键字:
    1,3: --type=spark
    2: --type=sqoop
    4: --type=blanca, 前提是conf文件中有 Jdbc | SparkJob
    5: Sqoop.py | HAML_PYTHON_SQOOP_TOOL
    6. sqoop/import/export/tidb

筛选病聚集所有可能的Job, 合并去重排序:

    project_home=~/Desktop/workspace/webank-aml
    output=~/Desktop/workspace/data_sync_jobs
    job_command_output=~/Desktop/workspace/data_sync_job_commands

    cd ${project_home}
    echo "" > ${output}

    grep -rlE 'type=spark|type=sqoop' aml_wtss/src/main/resources/wtss/bdp | sort -u >> ${output}

    confs=$(find aml_bdp/ -name "*.conf" | xargs grep -liEu "SparkJob|Jdbc" | xargs basename -a)
    echo ${confs} | tr ' ' '|' | xargs -I {} grep -rlE {} aml_wtss/src/main/resources/wtss/bdp | sort -u >> ${output}

    grep -rlE "Sqoop.py|HAML_PYTHON_SQOOP_TOOL" aml_wtss/src/main/resources/wtss/bdp | sort -u >> ${output}

    grep -rlE "sqoop|import|export|tidb" aml_wtss/src/main/resources/wtss/bdp | sort -u >> ${output}

    sed '/^$/d' ${output} | sort -u > ${output}

筛选所有的Job, 获取所有的command行, 获取程序入口

    cat ${output} | xargs grep -niE 'command.' > ${job_command_output}

手动依次查看所有的Job, 获取对应的资源配置


生产环境手动依次查看所有Job日志, 获取执行日志中的资源开销信息


抽离Job配置, 尽量聚集到配置文件中


修改配置文件参数, 测试和生产环境测试
