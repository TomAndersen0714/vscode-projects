# 此配置文件用于将指定文件的更新内容通过Kafka Channel输出到Kafka的指定topic中
# TailDir Source -> Kafka Channel

# Agent
a1.sources = r1
a1.channels = c1 c2
# 不使用Sink(Kafka Channel可以不使用Sink),主要是将数据采集到Kafka中去
# 当然可以使用Kafka Sink但是效率却没这种配置高,因为当前配置省去Sink

# Sources
# a1.sources.r1
a1.sources.r1.type = TAILDIR
# 设置Json文件存储路径(最好使用绝对路径)
# 用于记录文件inode/文件的绝对路径/每个文件的最后读取位置等信息
a1.sources.r1.positionFile = /opt/module/flume-1.8.0/.position/taildir_position.json
# 指定监控的文件组
a1.sources.r1.filegroups = f1
# 配置文件组中的文件
# 设置f1组的监控文件,注意:使用的是正则表达式,而不是Linux通配符
a1.sources.r1.filegroups.f1 = /tmp/logs/app.+
# 设置Event的Header中插入文件绝对路径键值对
a1.sources.r1.fileHeader = true


# Interceptors
# a1.sources.r1.interceptors
a1.sources.r1.interceptors = i1 i2
# 设置自定义的ETL Interceptor拦截器
a1.sources.r1.interceptors.i1.type = com.tomandersen.flume.interceptor.LogETLInterceptor$Builder
# 设置自定义的Log Type Interceptor拦截器
a1.sources.r1.interceptors.i2.type = com.tomandersen.flume.interceptor.LogTypeInterceptor$Builder


# Channel Selector
# a1.sources.r1.selector
a1.sources.r1.selector.type = multiplexing
# 设置Multiplexing Channel Selector根据日志类型发往不同的Channel
a1.sources.r1.selector.header = topic
# 将启动日志发往c1
a1.sources.r1.selector.mapping.topic_start = c1
# 将事件日志发往c2
a1.sources.r1.selector.mapping.topic_event = c2

# Channels
# a1.channels.c1
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
# 设置Kafka集群中的Broker
a1.channels.c1.kafka.bootstrap.servers = kafkaServer1:9092,kafkaServer2:9092,kafkaServer3:9092
# 设置a1.channels.c1所使用的Kafka的topic
a1.channels.c1.kafka.topic = topic_start
# 设置成不按照flume event格式解析数据,因为同一个Kafka topic可能有非flume Event类数据传入
a1.channels.c1.parseAsFlumeEvent = false
# # 设置注册的Kafka消费者组,此消费组应该设置成相同,保证同一个消费组中的用户两种数据都能读取
# a1.channels.c1.kafka.consumer.group.id = flume-consumer

# a1.channels.c2
a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
# 设置Kafka集群中的Broker
a1.channels.c2.kafka.bootstrap.servers = kafkaServer1:9092,kafkaServer2:9092,kafkaServer3:9092
# 设置a1.channels.c1所使用的Kafka的topic
a1.channels.c2.kafka.topic = topic_event
# 设置成不按照flume event格式解析数据,因为同一个Kafka topic可能有非flume Event类数据传入
a1.channels.c2.parseAsFlumeEvent = false
# # 设置注册的Kafka消费者组,此消费组应该设置成相同,保证同一个消费组中的用户两种数据都能读取
# a1.channels.c2.kafka.consumer.group.id = flume-consumer


# Bind
a1.sources.r1.channels = c1 c2