<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- Hive仓库路径,会自动加上hadoop配置参数fs.default.name作为前缀 -->
    <!-- 默认值:/user/hive/warehouse -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
        <description>location of default database for the warehouse</description>
    </property>
    <!-- 使用Hive CLI时是否将当前使用的数据库作为前缀输出 -->
    <!-- 默认值:false -->
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
        <description>Whether to include the current database in the Hive prompt.</description>
    </property>
    <!-- 使用Hive CLI时输出结果的同时是否输出列名 -->
    <!-- 默认值:false -->
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
        <description>Whether to print the names of the columns in query output.</description>
    </property>
    <!-- 设置是否进行元数据版本校验 -->
    <!-- 默认值:true -->
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
        <description>
            Enforce metastore schema version consistency.
            True: Verify that version information stored in is compatible with one from Hive jars.  Also disable automatic
                    schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures
                    proper metastore schema migration. (Default)
            False: Warn if the version information stored in metastore doesn't match with one from in Hive jars.
        </description>
    </property>

    <!-- 设置对于小数据,自动使用本地模式执行MR job,加快执行过程 -->
    <!-- 默认值:true -->
    <property>
        <name>hive.exec.mode.local.auto</name>
        <value>true</value>
        <description>Let Hive determine whether to run in local mode automatically</description>
    </property>
    <!-- 设置自动本地模式的输入数据量上限,大于此数据量则使用集群执行mr,反之则使用本地模式 -->
    <!-- 默认值:134217728 -->
    <property>
        <name>hive.exec.mode.local.auto.inputbytes.max</name>
        <value>134217728</value>
        <description>When hive.exec.mode.local.auto is true, input bytes should less than this for local mode.</description>
    </property>
    <!-- 设置自动本地模式的输入文件上限 -->
    <!-- 默认值:4 -->
    <property>
        <name>hive.exec.mode.local.auto.input.files.max</name>
        <value>4</value>
        <description>When hive.exec.mode.local.auto is true, the number of tasks should less than this for local mode.</description>
    </property>

    <!-- JDBC连接MySQL设置,其中MySQL安装在hadoop101上,MySQL通信端口为3306,
        存储Hive metadata的数据库名为hive_db,如果不存在对应数据库则进行创建 即createDatabaseIfNotExist=true-->
    <!-- 默认值:jdbc:derby:;databaseName=metastore_db;create=true -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop101:3306/hive_db?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
        <description>
            JDBC connect string for a JDBC metastore.
            To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
            For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
        </description>
    </property>
    <!-- Hive元数据库的驱动器 -->
    <!-- 默认值:org.apache.derby.jdbc.EmbeddedDriver -->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>
    <!-- 用于连接元数据数据库的用户名 -->
    <!-- 默认值:APP -->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
        <description>Username to use against metastore database</description>
    </property>
    <!-- 用于连接元数据的用户密码 -->
    <!-- 默认值:mine -->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>root</value>
        <description>password to use against metastore database</description>
    </property>

    <!-- 设置是否允许对单个分区列使用动态分区插入功能 -->
    <!-- 默认值:strict,既不允许-->
    <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
        <description>
            In strict mode, the user must specify at least one static partition
            in case the user accidentally overwrites all partitions.
            In nonstrict mode all partitions are allowed to be dynamic.
        </description>
    </property>

    <!-- 设置Hive查询检查,若为true,则使用ORDERBY子句时必须同时使用LIMIT子句,
        查询分区表时必须指定分区. -->
    <!-- 默认值:true -->
    <property>
        <name>hive.strict.checks.large.query</name>
        <value>false</value>
        <description>
            Enabling strict large query checks disallows the following:
                Orderby without limit.
                No partition being picked up for a query against partitioned table.
            Note that these checks currently do not consider data size, only the query pattern.
        </description>
    </property>

    <!-- 设置是否允许特定类型的比较,若为false,则不允许比较bigints和strings,不允许比较bigints和doubles -->
    <!-- 默认值:false -->
    <property>
        <name>hive.strict.checks.type.safety</name>
        <value>true</value>
        <description>
            Enabling strict type safety checks disallows the following:
                Comparing bigints and strings.
                Comparing bigints and doubles.
        </description>
    </property>

    <!-- 设置是否限制笛卡尔积查询 -->
    <!-- 一般情况下全局设置成true,如果确实需要查询笛卡尔积,可以在会话中手动设置局部变量解除
    当前会话的笛卡尔积查询限制 -->
    <property>
        <name>hive.strict.checks.cartesian.product</name>
        <value>true</value>
        <description>
            Enabling strict Cartesian join checks disallows the following:
                Cartesian product (cross join).
        </description>
    </property>

    <!-- 是否禁止向分桶表(bucketed table)中装载数据 -->
    <!-- 因为直接装载数据的过程只是拷贝数据文件到表中,这会破坏分桶表的逻辑结构,即
    会改变桶的数量,使得数据文件与元数据不符,因此建议禁止此类操作 -->
    <property>
        <name>hive.strict.checks.bucketing</name>
        <value>true</value>
        <description>
            Enabling strict bucketing checks disallows the following:
                Load into bucketed tables.
        </description>
    </property>

    <!-- 对于count(1)这类简单查询默认会从元数据库中查询统计信息,但统计数据默认只会
    在执行特定命令时才会统计行数等信息.在查询行数之前,可以先手动统计行数进行存入元数据库内
    便于之后直接查询-->
    <!-- 默认值:true -->
    <property>
        <name>hive.compute.query.using.stats</name>
        <value>false</value>
        <description>
            When set to true Hive will answer a few queries like count(1) purely using stats
            stored in metastore. For basic stats collection turn on the config hive.stats.autogather to true.
            For more advanced stats collection need to run analyze table queries.
        </description>
    </property>

    <!-- 设置默认使用mr引擎运行,因为在实验环境下数据量小,可以使用自动本地模式优化查询 -->
    <!-- 默认:mr -->
    <property>
        <name>hive.execution.engine</name>
        <value>mr</value>
        <description>
            Expects one of [mr, tez, spark].
            Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR
            remains the default engine for historical reasons, it is itself a historical engine
            and is deprecated in Hive 2 line. It may be removed without further warning.
        </description>
    </property>


    <!-- 以下是压缩功能的相关设置 -->

    <!-- 设置hive语句执行输出文件是否开启压缩,具体的压缩算法和压缩格式取决于hadoop中
    设置的相关参数 -->
    <!-- 默认值:false -->
    <property>
        <name>hive.exec.compress.output</name>
        <value>true</value>
        <description>
            This controls whether the final outputs of a query (to a local/HDFS file or a Hive table) 
            is compressed. 
            The compression codec and other options are determined from Hadoop config variables 
            mapred.output.compress*
        </description>
    </property>
    <!-- 控制多个MR Job的中间结果文件是否启用压缩,具体的压缩算法和压缩格式取决于hadoop中
    设置的相关参数 -->
    <!-- 默认值:false -->
    <property>
        <name>hive.exec.compress.intermediate</name>
        <value>true</value>
        <description>
            This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. 
            The compression codec and other options are determined from Hadoop config variables mapred.output.compress*
        </description>
    </property>
    <!-- 设置多个MR Job中间结果压缩使用的编解码器 -->
    <property>
        <name>hive.intermediate.compression.codec</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>
</configuration>