<configuration>

    <!--指定HDFS副本因子数-->
    <!--由于实验主机磁盘空间不足,本次实验中设置为1,一般需要设置为3-->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>

    <!--以下是关于HDFS(NameNode) HA模式的配置-->
    <!--设置集群的nameservice name-->
    <property>
        <name>dfs.nameservices</name>
        <value>dfsHAcluster</value>
    </property>

    <!--设置HDFS HA Cluster的节点-->
    <property>
        <name>dfs.ha.namenodes.dfsHAcluster</name>
        <value>nn1,nn2</value>
    </property>

    <!--关于HDFS HA集群nn1节点的配置-->
    <!--设置HDFS HA集群nn1节点的RPC地址-->
    <property>
        <name>dfs.namenode.rpc-address.dfsHAcluster.nn1</name>
        <value>hadoop101:8020</value>
    </property>

    <!--设置HDFS HA集群nn1节点的Web UI地址-->
    <property>
        <name>dfs.namenode.http-address.dfsHAcluster.nn1</name>
        <value>hadoop101:50070</value>
    </property>

        <!--指定HDFS HA集群dfsHAcluster中nn1节点上存储name table(fsimage)文件的本地路径-->
    <property>
        <name>dfs.namenode.name.dir.dfsHAcluster.nn1</name>
        <value>${hadoop.tmp.dir}/dfs/dfsHAcluster/nn1/fsimage</value>
    </property>

    <!--指定HDFS HA集群dfsHAcluster中nn1节点上存储transaction(edits)文件的本地路径-->
    <property>
        <name>dfs.namenode.edits.dir.dfsHAcluster.nn1</name>
        <value>${hadoop.tmp.dir}/dfs/dfsHAcluster/nn1/edits</value>
    </property>

    <!--指定HDFS HA集群dfsHAcluster中nn2节点上存储Blocks文件的本地路径-->
    <property>
        <name>dfs.datanode.data.dir.dfsHAcluster.nn1</name>
        <value>${hadoop.tmp.dir}/dfs/dfsHAcluster/nn1/data</value>
    </property>

    <!--关于HDFS HA集群nn2节点的配置-->
    <!--设置HDFS HA集群nn2节点的RPC地址-->
    <property>
        <name>dfs.namenode.rpc-address.dfsHAcluster.nn2</name>
        <value>hadoop102:8020</value>
    </property>

    <!--设置HDFS HA集群nn2节点的Web UI地址-->
    <property>
        <name>dfs.namenode.http-address.dfsHAcluster.nn2</name>
        <value>hadoop102:50070</value>
    </property>

    <!--指定HDFS HA集群dfsHAcluster中nn2节点上存储name table(fsimage)文件的本地路径-->
    <property>
        <name>dfs.namenode.name.dir.dfsHAcluster.nn2</name>
        <value>${hadoop.tmp.dir}/dfs/dfsHAcluster/nn2/fsimage</value>
    </property>

    <!--指定HDFS HA集群dfsHAcluster中nn2节点上存储transaction(edits)文件的本地路径-->
    <property>
        <name>dfs.namenode.edits.dir.dfsHAcluster.nn2</name>
        <value>${hadoop.tmp.dir}/dfs/dfsHAcluster/nn2/edits</value>
    </property>

    <!--指定HDFS HA集群dfsHAcluster中nn2节点上存储Blocks文件的本地路径-->
    <property>
        <name>dfs.datanode.data.dir.dfsHAcluster.nn2</name>
        <value>${hadoop.tmp.dir}/dfs/dfsHAcluster/nn2/data</value>
    </property>


    <!--设置Active NameNode向StandBy NameNode共享edits文件的URI-->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://zkServer1:8485;zkServer2:8485;zkServer3:8485/dfsHAcluster</value>
    </property>

    <!--设置HDFS用于联络Active NameNode的Java class-->
    <property>
        <name>dfs.client.failover.proxy.provider.dfsHAcluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

    <!--设置当进行故障转移failover时通过何种方式隔离Active NameNode-->
    <!--本次设置成使用ssh隔离-->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>

    <!--设置ssh隔离就必须设置成当前用户ssh对其他NameNode免密登录,
    同时需要在此提供私钥路径-->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/TomAndersen/.ssh/id_rsa</value>
    </property>

    <!--设置ssh连接超时时间-->
    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>30000</value>
    </property>

    <!--以下通过Zookeeper设置自动故障转移automatic failover-->
    <!--设置开启故障自动转移-->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>

</configuration>