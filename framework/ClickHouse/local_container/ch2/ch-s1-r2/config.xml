<?xml version="1.0"?>
<yandex>
    <tcp_port>29000</tcp_port>
    <http_port>8124</http_port>
    <interserver_http_port>9009</interserver_http_port>

    <macros>
        <layer>01</layer>
        <shard>01</shard>
        <replica>cluster01-01-02</replica>
    </macros>

    <!-- 集群配置 -->
    <remote_servers>
        <cluster_3s_2r>
            <!-- 数据分片1  -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>hadoop101</host>
                    <port>19000</port>
                </replica>
                <replica>
                    <host>hadoop102</host>
                    <port>29000</port>
                </replica>
            </shard>

            <!-- 数据分片2  -->
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>hadoop102</host>
                    <port>19000</port>
                </replica>
                <replica>
                    <host>hadoop101</host>
                    <port>29000</port>
                </replica>
            </shard>
        </cluster_3s_2r>
    </remote_servers>

    <!-- 存储策略 -->
    <storage_configuration>
        <disks>
            <data0>
                <path>/var/data/clickhouse-server/storage/data0/</path>
                <keep_free_space_bytes>1073741824</keep_free_space_bytes>
            </data0>
        </disks>

        <policies>
            <rr>
                <volumes>
                    <single>
                        <disk>data0</disk>
                    </single>
                </volumes>
            </rr>
        </policies>
    </storage_configuration>

    <!-- zk -->
    <zookeeper>
        <node>
            <host>hadoop101</host>
            <port>2182</port>
        </node>

        <node>
            <host>hadoop102</host>
            <port>2182</port>
        </node>

        <node>
            <host>hadoop103</host>
            <port>2182</port>
        </node>
    </zookeeper>

    <!-- 数据压缩算法  -->
    <compression>
        <case>
            <min_part_size>10000000000</min_part_size>
            <min_part_size_ratio>0.01</min_part_size_ratio>
            <method>lz4</method>
        </case>
    </compression>

    <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->
    <listen_host>::</listen_host>
    <listen_host>0.0.0.0</listen_host>
    <listen_try>1</listen_try>

    <logger>
        <level>trace</level>
        <log>/var/log/clickhouse-server/clickhouse-server.log</log>
        <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
        <size>1000M</size>
        <count>10</count>
    </logger>


    <max_connections>4096</max_connections>
    <keep_alive_timeout>3</keep_alive_timeout>
    <max_concurrent_queries>100</max_concurrent_queries>
    <uncompressed_cache_size>8589934592</uncompressed_cache_size>
    <mark_cache_size>5368709120</mark_cache_size>


    <path>/var/data/clickhouse-server/data/</path>
    <tmp_path>/var/data/clickhouse-server/data/tmp/</tmp_path>
    <user_files_path>/var/data/clickhouse-server/data/user_files/</user_files_path>
    <users_config>users.xml</users_config>
    <default_profile>default</default_profile>
    <default_database>default</default_database>
    <dictionaries_config>*_dictionary.xml</dictionaries_config>


    <timezone>Asia/Shanghai</timezone>
    <mlock_executable>true</mlock_executable>


    <builtin_dictionaries_reload_interval>3600</builtin_dictionaries_reload_interval>
    <max_session_timeout>3600</max_session_timeout>
    <default_session_timeout>60</default_session_timeout>


    <query_log>
        <database>system</database>
        <table>query_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_log>

    <trace_log>
        <database>system</database>
        <table>trace_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </trace_log>

    <query_thread_log>
        <database>system</database>
        <table>query_thread_log</table>
        <partition_by>toYYYYMM(event_date)</partition_by>
        <flush_interval_milliseconds>7500</flush_interval_milliseconds>
    </query_thread_log>

    <!-- Allow to execute distributed DDL queries (CREATE, DROP, ALTER, RENAME) on cluster.
         Works only if ZooKeeper is enabled. Comment it if such functionality isn't required. -->
    <distributed_ddl>
        <!-- Path in ZooKeeper to queue with DDL queries -->
        <path>/clickhouse/task_queue/ddl</path>
    </distributed_ddl>

    <!-- Directory in <clickhouse-path> containing schema files for various input formats.
         The directory will be created if it doesn't exist.-->
    <format_schema_path>/var/data/clickhouse-server/data/format_schemas/</format_schema_path>

</yandex>
