# flume1:此配置用于监控单个或多个指定文件将其追加内容先通过自定义的TypeInterceptor
# 向Header中添加type字段,然后使用Multiplexing Channel Selector将不同type的Event
# 传输到不同的Channel中,最后分别输出到flume2和flume3的控制台
# a1:TailDir Source-> TypeInterceptor -> Multiplexing Channel Selector ->
#   Memory Channel -> Avro Sink

# Agent
a1.sources = r1
a1.channels = c1 c2
a1.sinks = k1 k2

# Sources
# a1.sources.r1
a1.sources.r1.type = TAILDIR
# 设置Json文件存储路径(最好使用绝对路径)
# 用于记录文件inode/文件的绝对路径/每个文件的最后读取位置等信息
a1.sources.r1.positionFile = /opt/module/flume-1.8.0/.position/taildir_position.json
# 指定监控的文件组
a1.sources.r1.filegroups = f1
# 配置文件组中的被监控文件
# 设置f2组的监控文件,注意:使用的是正则表达式,而不是Linux通配符
a1.sources.r1.filegroups.f1 = /tmp/logs/^.*log$

# Interceptor
# a1.sources.r1.interceptors
# 配置Interceptor链,Interceptor调用顺序与配置循序相同
a1.sources.r1.interceptors = typeInterceptor
# 指定使用的自定义Interceptor全类名,并使用其中的静态内部类Builder
# 要想使用自定义Interceptor,必须将实现的类打包成jar包放入$FLUME_HOME/lib文件夹中
# flume运行Java程序时会将此路径加入到ClassPath中
a1.sources.r1.interceptors.typeInterceptor.type = com.tomandersen.interceptors.TypeInterceptor$Builder

# Channels
# a1.channels.c1
# 使用内存作为缓存/最多缓存的Event个数/单次传输的Event个数
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
# a1.channels.c2
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

# Channel Selector
# a1.sources.r1.selector
# 使用Multiple Channel Selector
a1.sources.r1.selector.type = multiplexing
# 设置匹配Header的字段
a1.sources.r1.selector.header = type
# 设置不同字段的值映射至各个Channel,将其余的Event丢弃
a1.sources.r1.selector.mapping.Startup = c1
a1.sources.r1.selector.mapping.Event = c2

# Sinks
# a1.sinks.k1
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = hadoop102
a1.sinks.k1.port = 4141
# a1.sinks.k2
a1.sinks.k2.type = avro
a1.sinks.k2.hostname = hadoop103
a1.sinks.k2.port = 4141

# Bind
# r1->TypeInterceptor->Multiplexing Channel Selector->c1->k1
# r1->TypeInterceptor->Multiplexing Channel Selector->c2->k2
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2
